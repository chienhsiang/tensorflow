{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import dataset_configs\n",
    "import data_io\n",
    "import u_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-alpha0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, losses\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_folder : 2019028023_PC9_A549_with_nuclear_marker\n",
      "img_file_pattern : *.png\n",
      "mask_file_pattern : *.png\n",
      "remove_pattern : _[A-Z]([2346789]|10|11)_\n",
      "crop_bd_width : 0\n",
      "n_channels : 1\n",
      "dtype : uint8\n",
      "batch_size : 5\n",
      "img_dir : /awlab/users/chsu/WorkSpace/tensorflow/segmentation/data/2019028023_PC9_A549_with_nuclear_marker/images\n",
      "mask_dir : /awlab/users/chsu/WorkSpace/tensorflow/segmentation/data/2019028023_PC9_A549_with_nuclear_marker/masks\n",
      "\n",
      "Number of training samples: 332\n",
      "Number of validation samples: 84\n"
     ]
    }
   ],
   "source": [
    "task = 'incucyte_nucleus'\n",
    "test_size = 0.2\n",
    "random_state = 423\n",
    "\n",
    "data_cfg = dataset_configs.get_dataset_config(task)\n",
    "data_cfg['remove_pattern'] = '_[A-Z]([2346789]|10|11)_'\n",
    "\n",
    "x_train_fnames, x_val_fnames, y_train_fnames, y_val_fnames = \\\n",
    "    data_io.get_data_filenames(**data_cfg, test_size=test_size, random_state=random_state)\n",
    "\n",
    "num_train_data = len(x_train_fnames)\n",
    "num_val_data = len(x_val_fnames)\n",
    "\n",
    "for k in data_cfg:\n",
    "    print(k, ':', data_cfg[k])\n",
    "print()\n",
    "print(\"Number of training samples: {}\".format(num_train_data))\n",
    "print(\"Number of validation samples: {}\".format(num_val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(num_val_data)\n",
    "print(os.path.basename(x_val_fnames[idx]))\n",
    "print(os.path.basename(y_val_fnames[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = re.compile('_(?P<well>[A-Z]\\d+)_')\n",
    "\n",
    "wells = [pattern.search(os.path.basename(f)).group('well') for f in x_train_fnames]\n",
    "\n",
    "print(len(set(wells)))\n",
    "print(sorted(list(set(wells))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_img_fn = functools.partial(data_io._get_image_from_path, channels=data_cfg['n_channels'], \n",
    "                                dtype=data_cfg['dtype'], crop_bd_width=data_cfg['crop_bd_width'])\n",
    "tmp_ds = data_io.get_dataset(x_train_fnames, y_train_fnames, read_img_fn=read_img_fn)\n",
    "\n",
    "plt.figure(figsize=(12,16))\n",
    "for i, (img, mask) in enumerate(tmp_ds.shuffle(num_train_data).take(3)):\n",
    "    print(img.numpy().max())\n",
    "    print(mask.numpy().max())\n",
    "    print(img.shape)\n",
    "    plt.subplot(3,2,2*i+1)\n",
    "    plt.imshow(img[0,:,:,0]/255., cmap='gray')\n",
    "    plt.subplot(3,2,2*i+2)\n",
    "    plt.imshow(mask[0,:,:,0]/255., cmap='binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_img_fn = functools.partial(data_io._get_image_from_path, channels=data_cfg['n_channels'], \n",
    "                                dtype=data_cfg['dtype'], crop_bd_width=data_cfg['crop_bd_width'])\n",
    "\n",
    "batch_size = data_cfg['batch_size']\n",
    "# batch_size = 5\n",
    "\n",
    "# training data\n",
    "train_cfg = {\n",
    "    'resize': None, \n",
    "    'scale': 1/255.,\n",
    "    'crop_size': [512, 512],\n",
    "    'to_flip': True\n",
    "}\n",
    "tr_preproc_fn = functools.partial(data_io._augment, **train_cfg)\n",
    "\n",
    "# validation data\n",
    "val_cfg = {\n",
    "    'resize': None, \n",
    "    'scale': 1/255.,\n",
    "    'crop_size': [512, 512]\n",
    "}\n",
    "val_preproc_fn = functools.partial(data_io._augment, **val_cfg)\n",
    "\n",
    "train_ds = data_io.get_dataset(x_train_fnames, y_train_fnames, read_img_fn=read_img_fn,\n",
    "                               preproc_fn=tr_preproc_fn, shuffle=True, batch_size=batch_size)\n",
    "val_ds = data_io.get_dataset(x_val_fnames, y_val_fnames, read_img_fn=read_img_fn, \n",
    "                             preproc_fn=val_preproc_fn, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,16))\n",
    "for i, (img, mask) in enumerate(val_ds.take(3)):\n",
    "    print(img.shape)\n",
    "\n",
    "    print(img.numpy().max())\n",
    "    print(mask.numpy().max())\n",
    "    \n",
    "    plt.subplot(3,3,3*i+1)\n",
    "    plt.imshow(img[0,:,:,0])\n",
    "    \n",
    "    plt.subplot(3,3,3*i+2)\n",
    "    plt.imshow(mask[0,:,:,0])\n",
    "    \n",
    "    \n",
    "    \n",
    "#     plt.subplot(3,3,3*i+3)\n",
    "#     plt.imshow(mask[0,:,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customized loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, mask in val_ds.shuffle(num_val_data).take(1):\n",
    "    idx = 0\n",
    "    \n",
    "    w_cfg = {\n",
    "        'nuc_ch': 1,\n",
    "        'cell_ch': 0,\n",
    "        'w0': 10,\n",
    "        'sigma': 3\n",
    "    }\n",
    "    \n",
    "    y_pred = model(img)\n",
    "    \n",
    "    loss, cb, nuc_dw, cell_dw = u_net.weighted_cce_loss(mask, y_pred, **w_cfg)\n",
    "    print(tf.reduce_max(cb, axis=(1,2)))\n",
    "    \n",
    "    print(loss.shape)\n",
    "    print(cb.shape)\n",
    "    print(nuc_dw.shape)\n",
    "    print(cell_dw.shape)\n",
    "    \n",
    "    L = loss[idx].numpy()\n",
    "    CB = cb[idx].numpy()\n",
    "    b = np.zeros_like(mask[idx,...,0])\n",
    "    NW = nuc_dw[idx].numpy()\n",
    "    CW = cell_dw[idx].numpy()\n",
    "    \n",
    "#     CB = CB / CB.max()\n",
    "#     NW = NW / np.median(NW)\n",
    "#     CW = CW / np.median(CW)\n",
    "    \n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.imshow(np.dstack([NW/NW.max(), mask[idx,...,w_cfg['nuc_ch']], b]))\n",
    "    \n",
    "    plt.subplot(2,2,2)\n",
    "    plt.imshow(np.dstack([CW/CW.max(), mask[idx,...,w_cfg['cell_ch']], b]))\n",
    "    \n",
    "    plt.subplot(2,2,3)\n",
    "    plt.imshow(np.dstack([L, mask[idx,...,w_cfg['cell_ch']], b]))\n",
    "    \n",
    "    print([CB.min(), CB.mean(), CB.max()])\n",
    "    print([NW.min(), NW.mean(), NW.max()])\n",
    "    print([CW.min(), CW.mean(), CW.max()])\n",
    "    print([L.min(), L.mean(), L.max()])\n",
    "    \n",
    "#     plt.subplot(2,2,3)\n",
    "#     plt.hist(NW, bins='auto')\n",
    "    \n",
    "#     plt.subplot(2,2,4)\n",
    "#     plt.hist(CW, bins='auto')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(u_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (img, mask) in val_ds.shuffle(30).take(1):\n",
    "    print(mask.dtype)\n",
    "#     L = u_net.weighted_loss(mask, mask)\n",
    "    M = mask[0,:,:,0]\n",
    "    wc = u_net.balancing_weight_tf(M)\n",
    "    dw = u_net.distance_weight(M.numpy(), w0=5, sigma=3.0)\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(12,16))\n",
    "    b = np.zeros_like(M)\n",
    "    plt.imshow(np.dstack([dw/dw.max(), M, b]))\n",
    "#     plt.imshow(dw)\n",
    "    \n",
    "#     plt.subplot(1,3,1)\n",
    "#     plt.imshow(img[0,:,:,0])\n",
    "\n",
    "#     plt.subplot(1,3,2)\n",
    "#     plt.imshow(M)\n",
    "\n",
    "#     plt.subplot(1,3,3)\n",
    "#     plt.imshow(dw)\n",
    "    \n",
    "#     print(L.shape)\n",
    "    print((wc.numpy().min(), wc.numpy().max()))\n",
    "    print((dw.min(), dw.max()))\n",
    "    \n",
    "#     plt.figure(figsize=(12,16))\n",
    "    \n",
    "#     for i, w in enumerate(L):\n",
    "#         if i>2:\n",
    "#             break\n",
    "            \n",
    "#         plt.subplot(3,3,3*i+1)\n",
    "#         plt.imshow(img[i,:,:,0])\n",
    "\n",
    "#         plt.subplot(3,3,3*i+2)\n",
    "#         plt.imshow(mask[i,:,:,0])\n",
    "\n",
    "#         plt.subplot(3,3,3*i+3)\n",
    "#         plt.imshow(w)\n",
    "#         print((w.numpy().min(), w.numpy().max()))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters_list = [32, 64, 128, 256, 512]\n",
    "n_classes = 2\n",
    "\n",
    "w_cfg = {\n",
    "    'nuc_ch': 1,\n",
    "    'cell_ch': 0,\n",
    "    'w0': 5,\n",
    "    'sigma': 3\n",
    "}\n",
    "\n",
    "model = u_net.Unet(num_filters_list, n_classes=n_classes, dynamic=True)\n",
    "\n",
    "loss_fn = functools.partial(u_net.weighted_loss, w0=w_cfg['w0'], sigma=w_cfg['sigma'])\n",
    "loss_fn = losses.binary_crossentropy\n",
    "# loss_fn = functools.partial(u_net.weighted_cce_loss, **w_cfg)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(tf.random.uniform([1, 512, 512, 1]))\n",
    "print(y.shape)\n",
    "plt.imshow(y[0,...,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = r'/awlab/users/chsu/WorkSpace/tensorflow/segmentation'\n",
    "model_tag = 'incucyte_nuc_weighted_'\n",
    "\n",
    "timestamp = '{}'.format(datetime.datetime.now()).split('.')[0].replace(':','_').replace(' ','_')\n",
    "foler_name = model_tag + timestamp\n",
    "\n",
    "# model weights\n",
    "weights_path = os.path.join(root_path, 'models', foler_name, 'weights-{epoch:04d}.ckpt')\n",
    "weights_dir = os.path.dirname(weights_path)\n",
    "if not os.path.isdir(weights_dir):\n",
    "    os.makedirs(weights_dir, exist_ok=True)\n",
    "cp = tf.keras.callbacks.ModelCheckpoint(filepath=weights_path, monitor='val_loss', \n",
    "                                        save_best_only=True, save_weights_only=True, verbose=1)\n",
    "# tensorboard\n",
    "log_dir = os.path.join(root_path, 'logs', foler_name)\n",
    "if not os.path.isdir(log_dir):\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "tb = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0423 15:09:11.329837 139983286044416 callbacks.py:1218] TensorBoard Callback will ignore `write_graph=True`when `Model.run_eagerly=True`.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.2729\n",
      "Epoch 00001: val_loss improved from inf to 0.34693, saving model to /awlab/users/chsu/WorkSpace/tensorflow/segmentation/models/incucyte_nuc_weighted_2019-04-23_15_09_09/weights-0001.ckpt\n",
      "67/67 [==============================] - 54s 801ms/step - loss: 0.2713 - val_loss: 0.3469\n",
      "Epoch 2/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.1309\n",
      "Epoch 00002: val_loss improved from 0.34693 to 0.30712, saving model to /awlab/users/chsu/WorkSpace/tensorflow/segmentation/models/incucyte_nuc_weighted_2019-04-23_15_09_09/weights-0002.ckpt\n",
      "67/67 [==============================] - 54s 803ms/step - loss: 0.1305 - val_loss: 0.3071\n",
      "Epoch 3/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00003: val_loss did not improve from 0.30712\n",
      "67/67 [==============================] - 50s 746ms/step - loss: 0.0970 - val_loss: 0.3403\n",
      "Epoch 4/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0850\n",
      "Epoch 00004: val_loss did not improve from 0.30712\n",
      "67/67 [==============================] - 49s 736ms/step - loss: 0.0851 - val_loss: 0.4061\n",
      "Epoch 5/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0767\n",
      "Epoch 00005: val_loss improved from 0.30712 to 0.21151, saving model to /awlab/users/chsu/WorkSpace/tensorflow/segmentation/models/incucyte_nuc_weighted_2019-04-23_15_09_09/weights-0005.ckpt\n",
      "67/67 [==============================] - 54s 809ms/step - loss: 0.0766 - val_loss: 0.2115\n",
      "Epoch 6/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0701\n",
      "Epoch 00006: val_loss improved from 0.21151 to 0.11487, saving model to /awlab/users/chsu/WorkSpace/tensorflow/segmentation/models/incucyte_nuc_weighted_2019-04-23_15_09_09/weights-0006.ckpt\n",
      "67/67 [==============================] - 54s 807ms/step - loss: 0.0702 - val_loss: 0.1149\n",
      "Epoch 7/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0680\n",
      "Epoch 00007: val_loss did not improve from 0.11487\n",
      "67/67 [==============================] - 49s 739ms/step - loss: 0.0679 - val_loss: 0.2596\n",
      "Epoch 8/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0661\n",
      "Epoch 00008: val_loss did not improve from 0.11487\n",
      "67/67 [==============================] - 50s 743ms/step - loss: 0.0662 - val_loss: 0.1426\n",
      "Epoch 9/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0634\n",
      "Epoch 00009: val_loss did not improve from 0.11487\n",
      "67/67 [==============================] - 49s 735ms/step - loss: 0.0634 - val_loss: 0.1221\n",
      "Epoch 10/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0614\n",
      "Epoch 00010: val_loss improved from 0.11487 to 0.10304, saving model to /awlab/users/chsu/WorkSpace/tensorflow/segmentation/models/incucyte_nuc_weighted_2019-04-23_15_09_09/weights-0010.ckpt\n",
      "67/67 [==============================] - 54s 803ms/step - loss: 0.0616 - val_loss: 0.1030\n",
      "Epoch 11/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0604\n",
      "Epoch 00011: val_loss did not improve from 0.10304\n",
      "67/67 [==============================] - 46s 693ms/step - loss: 0.0602 - val_loss: 0.1365\n",
      "Epoch 12/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0579\n",
      "Epoch 00012: val_loss improved from 0.10304 to 0.09634, saving model to /awlab/users/chsu/WorkSpace/tensorflow/segmentation/models/incucyte_nuc_weighted_2019-04-23_15_09_09/weights-0012.ckpt\n",
      "67/67 [==============================] - 51s 756ms/step - loss: 0.0580 - val_loss: 0.0963\n",
      "Epoch 13/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0592\n",
      "Epoch 00013: val_loss did not improve from 0.09634\n",
      "67/67 [==============================] - 46s 690ms/step - loss: 0.0593 - val_loss: 0.7663\n",
      "Epoch 14/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0570\n",
      "Epoch 00014: val_loss did not improve from 0.09634\n",
      "67/67 [==============================] - 47s 700ms/step - loss: 0.0569 - val_loss: 0.2270\n",
      "Epoch 15/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0578\n",
      "Epoch 00015: val_loss did not improve from 0.09634\n",
      "67/67 [==============================] - 47s 703ms/step - loss: 0.0576 - val_loss: 0.2405\n",
      "Epoch 16/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0547\n",
      "Epoch 00016: val_loss did not improve from 0.09634\n",
      "67/67 [==============================] - 47s 700ms/step - loss: 0.0547 - val_loss: 0.1981\n",
      "Epoch 17/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0558\n",
      "Epoch 00017: val_loss did not improve from 0.09634\n",
      "67/67 [==============================] - 47s 700ms/step - loss: 0.0557 - val_loss: 0.1160\n",
      "Epoch 18/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0537\n",
      "Epoch 00018: val_loss did not improve from 0.09634\n",
      "67/67 [==============================] - 47s 709ms/step - loss: 0.0536 - val_loss: 0.4698\n",
      "Epoch 19/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0541\n",
      "Epoch 00019: val_loss did not improve from 0.09634\n",
      "67/67 [==============================] - 47s 694ms/step - loss: 0.0540 - val_loss: 0.0998\n",
      "Epoch 20/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0528\n",
      "Epoch 00020: val_loss improved from 0.09634 to 0.08892, saving model to /awlab/users/chsu/WorkSpace/tensorflow/segmentation/models/incucyte_nuc_weighted_2019-04-23_15_09_09/weights-0020.ckpt\n",
      "67/67 [==============================] - 51s 757ms/step - loss: 0.0528 - val_loss: 0.0889\n",
      "Epoch 21/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0539\n",
      "Epoch 00021: val_loss did not improve from 0.08892\n",
      "67/67 [==============================] - 47s 699ms/step - loss: 0.0541 - val_loss: 0.0935\n",
      "Epoch 22/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0526\n",
      "Epoch 00022: val_loss did not improve from 0.08892\n",
      "67/67 [==============================] - 46s 688ms/step - loss: 0.0523 - val_loss: 0.5969\n",
      "Epoch 23/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0519\n",
      "Epoch 00023: val_loss did not improve from 0.08892\n",
      "67/67 [==============================] - 47s 700ms/step - loss: 0.0519 - val_loss: 0.2170\n",
      "Epoch 24/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0526\n",
      "Epoch 00024: val_loss did not improve from 0.08892\n",
      "67/67 [==============================] - 48s 710ms/step - loss: 0.0526 - val_loss: 0.0923\n",
      "Epoch 25/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0512\n",
      "Epoch 00025: val_loss did not improve from 0.08892\n",
      "67/67 [==============================] - 47s 702ms/step - loss: 0.0512 - val_loss: 0.1696\n",
      "Epoch 26/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0500\n",
      "Epoch 00026: val_loss did not improve from 0.08892\n",
      "67/67 [==============================] - 48s 711ms/step - loss: 0.0498 - val_loss: 0.4678\n",
      "Epoch 27/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0513\n",
      "Epoch 00027: val_loss did not improve from 0.08892\n",
      "67/67 [==============================] - 47s 697ms/step - loss: 0.0514 - val_loss: 0.1400\n",
      "Epoch 28/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0511\n",
      "Epoch 00028: val_loss did not improve from 0.08892\n",
      "67/67 [==============================] - 48s 714ms/step - loss: 0.0509 - val_loss: 0.3958\n",
      "Epoch 29/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0504\n",
      "Epoch 00029: val_loss did not improve from 0.08892\n",
      "67/67 [==============================] - 47s 704ms/step - loss: 0.0503 - val_loss: 0.1843\n",
      "Epoch 30/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0498\n",
      "Epoch 00030: val_loss improved from 0.08892 to 0.07136, saving model to /awlab/users/chsu/WorkSpace/tensorflow/segmentation/models/incucyte_nuc_weighted_2019-04-23_15_09_09/weights-0030.ckpt\n",
      "67/67 [==============================] - 51s 762ms/step - loss: 0.0498 - val_loss: 0.0714\n",
      "Epoch 31/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0519\n",
      "Epoch 00031: val_loss did not improve from 0.07136\n",
      "67/67 [==============================] - 46s 687ms/step - loss: 0.0521 - val_loss: 0.1641\n",
      "Epoch 32/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0530\n",
      "Epoch 00032: val_loss did not improve from 0.07136\n",
      "67/67 [==============================] - 46s 687ms/step - loss: 0.0530 - val_loss: 0.0778\n",
      "Epoch 33/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0489\n",
      "Epoch 00033: val_loss did not improve from 0.07136\n",
      "67/67 [==============================] - 49s 738ms/step - loss: 0.0488 - val_loss: 0.1188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0502\n",
      "Epoch 00034: val_loss did not improve from 0.07136\n",
      "67/67 [==============================] - 46s 692ms/step - loss: 0.0503 - val_loss: 3.7621\n",
      "Epoch 35/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0516\n",
      "Epoch 00035: val_loss did not improve from 0.07136\n",
      "67/67 [==============================] - 48s 714ms/step - loss: 0.0516 - val_loss: 0.2398\n",
      "Epoch 36/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0469\n",
      "Epoch 00036: val_loss did not improve from 0.07136\n",
      "67/67 [==============================] - 47s 695ms/step - loss: 0.0470 - val_loss: 0.0725\n",
      "Epoch 37/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0498\n",
      "Epoch 00037: val_loss did not improve from 0.07136\n",
      "67/67 [==============================] - 46s 693ms/step - loss: 0.0498 - val_loss: 0.0882\n",
      "Epoch 38/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0505\n",
      "Epoch 00038: val_loss did not improve from 0.07136\n",
      "67/67 [==============================] - 48s 717ms/step - loss: 0.0506 - val_loss: 0.0757\n",
      "Epoch 39/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0480\n",
      "Epoch 00039: val_loss did not improve from 0.07136\n",
      "67/67 [==============================] - 47s 695ms/step - loss: 0.0479 - val_loss: 0.0796\n",
      "Epoch 40/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0473\n",
      "Epoch 00040: val_loss did not improve from 0.07136\n",
      "67/67 [==============================] - 46s 687ms/step - loss: 0.0472 - val_loss: 0.5387\n",
      "Epoch 41/50\n",
      "66/67 [============================>.] - ETA: 0s - loss: 0.0491\n",
      "Epoch 00041: val_loss improved from 0.07136 to 0.05718, saving model to /awlab/users/chsu/WorkSpace/tensorflow/segmentation/models/incucyte_nuc_weighted_2019-04-23_15_09_09/weights-0041.ckpt\n",
      "67/67 [==============================] - 50s 750ms/step - loss: 0.0490 - val_loss: 0.0572\n",
      "Epoch 42/50\n",
      "48/67 [====================>.........] - ETA: 12s - loss: 0.0497"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=50, \n",
    "                    steps_per_epoch=int(np.ceil(num_train_data / batch_size)),\n",
    "                    validation_data=val_ds,\n",
    "                    validation_steps=int(np.ceil(num_val_data / batch_size)),\n",
    "                    callbacks=[cp, tb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_process = pd.DataFrame.from_dict(history.history)\n",
    "tr_process['epoch'] = np.array(range(1, tr_process.shape[0]+1))\n",
    "\n",
    "tr_process.plot(x='epoch', y=['loss', 'val_loss'])\n",
    "# tr_process.plot(x='epoch', y=['dice_loss', 'val_dice_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or load the trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = r'/awlab/users/chsu/WorkSpace/tensorflow/segmentation/models/nucleus_weighted_bce22019-04-16_07_38_00'\n",
    "latest = tf.train.latest_checkpoint(model_dir)\n",
    "print(latest)\n",
    "model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(val_ds, steps=int(np.ceil(num_val_data / batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_mask(I, M, M_pred, true_color=(0,255,0), pred_color=(255,0,0)):\n",
    "    \"\"\"I, M, M_pred are uint8 numpy arrays\n",
    "    \"\"\"\n",
    "    if I.shape[-1] == 1:\n",
    "        I = cv2.cvtColor(I,cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    im, contours, _ = cv2.findContours(M.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    im_pred, contours_pred, _ = cv2.findContours(M_pred.copy(), \n",
    "                                                 cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    Z = np.zeros_like(I)\n",
    "    if true_color is None:\n",
    "        I1 = np.zeros_like(I)\n",
    "    else:\n",
    "        I1 = cv2.drawContours(Z.copy(), contours, -1, true_color, 1)\n",
    "        \n",
    "    I2 = cv2.drawContours(Z.copy(), contours_pred, -1, pred_color, 1)\n",
    "    \n",
    "    I = np.uint8(np.clip(np.float32(I) + np.float32(I1) + np.float32(I2), 0, 255))\n",
    "    \n",
    "    return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_plot = np.random.choice(num_val_data, 3)\n",
    "result_folder = r'/awlab/users/chsu/WorkSpace/tensorflow/segmentation/results'\n",
    "nuc_idx = 1\n",
    "cell_idx = 0\n",
    "\n",
    "if not os.path.isdir(result_folder):\n",
    "    os.makedirs(result_folder)\n",
    "\n",
    "# plt.figure(figsize=(12,16))\n",
    "\n",
    "for i, (img, mask) in enumerate(val_ds):\n",
    "    if i in idx_to_plot:\n",
    "        y_pred = model(img)        \n",
    "        for j in range(4):\n",
    "            I = np.uint8(img[j].numpy()*255.)\n",
    "            M = np.uint8(mask[j].numpy()*255.) \n",
    "            M_pred = np.uint8((y_pred[j].numpy() > 0.5) *255.)\n",
    "            \n",
    "            if task == 'both_seg':\n",
    "                I = np.uint8(img[j].numpy()*255.)\n",
    "                M = np.uint8(mask[j].numpy()*255.) \n",
    "                M_pred = np.uint8((y_pred[j].numpy() > 0.5) *255.)\n",
    "                \n",
    "                # overlay nucleus segmentation\n",
    "                I = overlay_mask(I, M[:,:,nuc_idx], M_pred[:,:,nuc_idx], \n",
    "                                 true_color=None, pred_color=(0,255,255))\n",
    "                # overlay cell segmentation\n",
    "                I = overlay_mask(I, M[:,:,cell_idx], M_pred[:,:,cell_idx], \n",
    "                                 true_color=None, pred_color=(255,0,255))\n",
    "            else:\n",
    "                I = overlay_mask(I, M[:,:,0], M_pred[:,:,0])\n",
    "            \n",
    "            fname = os.path.join(result_folder, '{}_{}.png'.format(i,j))\n",
    "            cv2.imwrite(fname, cv2.cvtColor(I, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "#             plt.subplot(2,2,j+1)\n",
    "#             plt.imshow(I)\n",
    "            \n",
    "    if i > max(idx_to_plot):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# plt.figure(figsize=(16, 16))\n",
    "for img, mask in val_ds.shuffle(num_val_data).take(1):\n",
    "    idx = 0\n",
    "    y_pred = model(img)\n",
    "    \n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(img[idx])\n",
    "    \n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(y_pred[idx,:,:,0], cmap='gray')\n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(y_pred[idx,:,:,1], cmap='gray')\n",
    "    \n",
    "#     print(mask[idx].numpy().sum(axis=(0,1)))\n",
    "#     print(mask[idx].numpy().sum())\n",
    "    print(y_pred[idx].numpy().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(data_io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "# plt.figure(figsize=(16, 16))\n",
    "\n",
    "idx = np.random.choice(num_val_data)\n",
    "img = cv2.imread(x_val_fnames[idx]) \n",
    "mask = cv2.imread(y_val_fnames[idx])\n",
    "\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.\n",
    "mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB) / 255.\n",
    "\n",
    "img2 = tf.image.decode_png(tf.io.read_file(x_val_fnames[idx]), channels=3, dtype=tf.uint16)\n",
    "img2 = tf.cast(img2, tf.float32) / 255.\n",
    "# img2 = tf.image.convert_image_dtype(img2, tf.uint8, saturate=True)\n",
    "img2 = img2.numpy()\n",
    "\n",
    "print(img2.dtype, img2.min(), img2.max())\n",
    "\n",
    "plt.figure\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2)\n",
    "\n",
    "# print(img.shape, mask.shape)\n",
    "# print(img.dtype, img.min(), img.max())\n",
    "# print(mask.dtype, mask.min(), mask.max())\n",
    "# \n",
    "\n",
    "# y_pred = model(img)\n",
    "\n",
    "# plt.subplot(1,3,1)\n",
    "# plt.imshow(img[idx])\n",
    "\n",
    "# plt.subplot(1,3,2)\n",
    "# plt.imshow(y_pred[idx,:,:,0], cmap='gray')\n",
    "\n",
    "# plt.subplot(1,3,3)\n",
    "# plt.imshow(y_pred[idx,:,:,1], cmap='gray')\n",
    "    \n",
    "#     print(mask[idx].numpy().sum(axis=(0,1)))\n",
    "#     print(mask[idx].numpy().sum())\n",
    "#     print(y_pred[idx].numpy().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, mask in val_ds.shuffle(num_val_data).take(1):\n",
    "    idx = 0\n",
    "    y_pred = model(img)\n",
    "    N = tf.reduce_sum(mask, axis=(1,2))\n",
    "    W = tf.expand_dims(1 / N, axis=1)\n",
    "    W = tf.expand_dims(W, axis=2)\n",
    "    weight = mask * W / mask.shape[-1]\n",
    "    print(tf.reduce_sum(weight, axis=(1,2,3)))\n",
    "#     plt.imshow(weight[0] / weight[0].numpy().max())\n",
    "#     print(tf.unique(tf.reshape(weight[0], [-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fname = r'/awlab/users/chsu/WorkSpace/tensorflow/segmentation/data/plate_2017017086_ki67/images/CH_171219_Vh31_Goldilocks_plate_2017017086_ki67_B8_0001-2.png'\n",
    "\n",
    "img = tf.image.decode_image(tf.io.read_file(fname), channels=3, dtype=tf.uint8)\n",
    "# img = tf.cast(img, tf.float32) / 4095.\n",
    "# img = tf.image.convert_image_dtype(img, tf.uint8, saturate=True)\n",
    "print(img.shape)\n",
    "img = img.numpy()\n",
    "print(img.min(), img.max())\n",
    "\n",
    "plt.figure\n",
    "plt.imshow(img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
