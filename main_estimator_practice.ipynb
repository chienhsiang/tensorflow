{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geneal steps:\n",
    "    1. Write dataset importing function(s).\n",
    "    2. Define the feature columns.\n",
    "    3. Instantiate the Estimator.\n",
    "    4. Call a training, evaluation, or inference method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bucketized feature column\n",
    "numeric_feature_column = tf.feature_column.numeric_column('Year')\n",
    "bucketized_feature_column = tf.feature_column.bucketized_column(\n",
    "    source_column=numeric_feature_column, boundaries=[1960, 1980, 2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a categorical identity column (one-hot code) for integers\n",
    "identity_feature_column = tf.feature_column.categorical_column_with_identity(\n",
    "    key='my_feature', num_buckets=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary column for strings\n",
    "vocabulary_feature_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key='feature_name',vocabulary_list=['apple','cake','dog'])\n",
    "\n",
    "vocabulary_feature_column = tf.feature_column.categorical_column_with_vocabulary_file(\n",
    "    key='feature_name',vocabulary_file='file_name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hashed column\n",
    "hashed_feature_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    key='some_name', hash_bucket_size=100) # The number of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crossed column (interaction term of two categorical variables)\n",
    "tf.feature_column.crossed_column()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.feature_column.indicator_column()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset fro Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/home/chsu/workspace/tensorflow/models/samples/core/get_started')\n",
    "\n",
    "import iris_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = iris_data.load_data()\n",
    "features, labels = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: {SepalLength: (), SepalWidth: (), PetalLength: (), PetalWidth: ()}, types: {SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}>\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(dict(features))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ({SepalLength: (), SepalWidth: (), PetalLength: (), PetalWidth: ()}, ()), types: ({SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ({SepalLength: (?,), SepalWidth: (?,), PetalLength: (?,), PetalWidth: (?,)}, (?,)), types: ({SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle(1000).repeat().batch(32)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chsu/.keras/datasets/iris_training.csv\n",
      "/home/chsu/.keras/datasets/iris_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Build a dataset by reading a csv file\n",
    "train_path, test_path = iris_data.maybe_download()\n",
    "print(train_path)\n",
    "print(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.TextLineDataset(train_path).skip(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.decode_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(features, labels, mode, params):\n",
    "    # 1. Define the input layer\n",
    "    net = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "    \n",
    "    # 2. Make hidden layers\n",
    "    for units in params['hidden_units']:\n",
    "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
    "        \n",
    "    # 3. Make the output layer (logits, one per class)\n",
    "    logits = tf.layers.dense(net, units=params['n_classes'], activation=None)\n",
    "    \n",
    "    # Prediction\n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': predicted_classes[:, tf.newaxis],\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "    # Evaluation\n",
    "    accuracy = tf.metrics.accuracy(labels, predicted_classes, name='acc_op')\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss==loss, eval_metric_ops=metrics)\n",
    "    \n",
    "    # Training\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "    \n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
